==========================================================================
QuickSort

- "greatest hit" algorithm
- prevalent in practice
- beautiful analysis
- O(n log n) time "on average", works in place 
	- i.e. minimal extra memory needed


The Sorting Problem: 

Input: array of n numbers, unsorted
Output: same numbers, sorted in increasing order
Assume: all array entries distinct
Exercise: extend quicksort to handle duplicate entries

[3, 8, 2, 5, 1, 4, 7, 6]

Partitioning Around a Pivot

Key idea: parition array around a "pivot element"

- pick element of array (3, idx = 0)

- rearrange array so that:
	- left of pivot => less than pivot
	- right of pivot => greater than pivot
	
[2, 1, 3, 6, 7, 4, 5 ,8]

idx 0, 1 < pivot
idx > 2, > pivot

Note: puts pivot in its "rightful position"

	- third largest element winds up in third position of the array
	

Two Cool Facts About Partition

	1) linear time (O(n)), no extra memory
		[see next video]
		
	2) reduces problem size (enables divide and conquer approach)
	


QuickSort: High-Level Description

	Tony Hoore, circa 1961

	Beginning of career, 26-27
	

QuickSort( arrary A, length n ):

	if n = 1:
		return
		
	// choose pivot element
	p = ChoosePivot(A, n)  // unimplemented
	
	// partition array around pivot element
	// partition A around p
	firstPart, secondPart = MakePartition()
	
	// less than p, first recursive call
	firstPart = QuickSort()
	
	// greater than p, second recursive call
	secondPart = QuickSort()
	

====================================================================================	
Partitioning Around a Pivot


Key Idea: partition array around a pivot element

	- pick element of array (say, first)
	
	- ensure everything less then is to the left
	
	- ensure everything greater than is to the right
	

Two Cool Facts About Partition

1) linear O(n) time, no extra memory [see next video]
2) reduceds problem size


Partitioning Subroutine Implementation

Easy Way Out: using O(n) w/ extra memory, easy to partition around pivot in O(n) time

[3, 8, 2, 5, 1, 4, 7, 6]

Linear scan and bucket less than and greater than

Add to the left (idx ++) and to the right (idx --) of a copy of the allocated array


In-Place Impelementation

Assume: pivot = 1st element of the array

NOTE:
[if not, swap pivot <-> 1st element as preprocessing step]

High-Level Idea: two groups, what we've seen and what we haven't seen

Split what we've seen into [less than, greater than]

Keep pivot at idx 0 until the very end

all elements less than the pivot come first, all elements greater than the pivot come last

- single scan through array
- invariant: everything looked at so far is partitioned

Keep track of:
	- boundary of what we have looked at / haven't looked at (j)
	- where is the split for less than / greater than pivot (i)
	
Each step must advance j to maintain linear O(n)

The first examined element is indeed partitioned (no comparison necessary)

If the first element is bigger then pivot, i remains the same

If any subsequent element is smaller, we must swap (another routine, perhaps)


==============================================================================
Choosing a Good Pivot


Running time of QuickSort depends on the quality of the pivot


If we implement QuickSort so that ChoosePivot always selects the first element of the array, the running time is quadratic on an input array that is already sorted (O(n^2)), which is VERY BAD for a sorting algorithm

The lower bound on this algorithm is quadratic because we recurse on every element in the array (using this method of choosing the pivot)



We ideally want the median element (n/2 split for recursion) - best case scenario

If in every recursive call we get a 50/50 split, the running time is O(nlogn)

The recurrence matches the recurrence that governs MergeSort (nlogn)

Since the recursive calls are at most n/2, T(n) <= 2T(n/2)

Subproblems shrink at the size of subproblem proliferation

Since we have a tight recurrence on partition (theta n) we actually get a tighter estimate (theta nlogn) in this magical scenario of perfect pivots


Random Pivots

QuickSort is the first example of randomized algorithms (e.g. flipping a coin to get performance)

Big Idea: RANDOM PIVOTS

Each input in array of length k has the same odds of selection

This algorithm will run differently on the same input on different runs

In the 1970s it was discovered that randomness could make solving certain problems very fast, more elegant and easier to maintain

Hope: a random pivot is "pretty good" "often enough"

Any pivot that gets us a reasonable split (close to n/2) will yield nlogn time


Two intuitive points:

1: A 25/75 split is defined as good enough for nlogn (claim by Roughgarden)

Because we do not have balanced sub-problems, you cannot use Master Method to analyze this

2: We don't have to get that lucky to get a 25/75 split
i.e. in an array of 1-100 we can pick anything such that 25 >= x >= 75

50% probability we get a split that is "good enough"



Mathematical Analysis
If you really want to know if an idea is good or bad, you need mathematical analysis


Quicksort Theorem: For every input array of length n, the average running time of QuickSort (with random pivots) is O(nlogn)

NOTE: every input (no assumptions about the data)

- recall our guiding principles - general principles with great guarantees independent of the data

If you have a dataset that fits in the memory of your machine, sorting is a computational primitive - QuickSort runs fast enough that you can think of it as "for free"

Go ahead and sort the array - it might make your life easier in other algorithmic or computational problems

NOTE: "average" is over random choices made by the algorithm 
(i.e. pivot choices)

We've seen QuickSort vary between upper bound O(n^2) and lower bound O(nlogn). What is amazing about QuickSort, you are almost never going to see anything but the lower bound thanks to randomization









