==========================================================================
QuickSort

- "greatest hit" algorithm
- prevalent in practice
- beautiful analysis
- O(n log n) time "on average", works in place 
	- i.e. minimal extra memory needed


The Sorting Problem: 

Input: array of n numbers, unsorted
Output: same numbers, sorted in increasing order
Assume: all array entries distinct
Exercise: extend quicksort to handle duplicate entries

[3, 8, 2, 5, 1, 4, 7, 6]

Partitioning Around a Pivot

Key idea: parition array around a "pivot element"

- pick element of array (3, idx = 0)

- rearrange array so that:
	- left of pivot => less than pivot
	- right of pivot => greater than pivot
	
[2, 1, 3, 6, 7, 4, 5 ,8]

idx 0, 1 < pivot
idx > 2, > pivot

Note: puts pivot in its "rightful position"

	- third largest element winds up in third position of the array
	

Two Cool Facts About Partition

	1) linear time (O(n)), no extra memory
		[see next video]
		
	2) reduces problem size (enables divide and conquer approach)
	


QuickSort: High-Level Description

	Tony Hoore, circa 1961

	Beginning of career, 26-27
	

QuickSort( arrary A, length n ):

	if n = 1:
		return
		
	// choose pivot element
	p = ChoosePivot(A, n)  // unimplemented
	
	// partition array around pivot element
	// partition A around p
	firstPart, secondPart = MakePartition()
	
	// less than p, first recursive call
	firstPart = QuickSort()
	
	// greater than p, second recursive call
	secondPart = QuickSort()
	

====================================================================================	
Partitioning Around a Pivot


Key Idea: partition array around a pivot element

	- pick element of array (say, first)
	
	- ensure everything less then is to the left
	
	- ensure everything greater than is to the right
	

Two Cool Facts About Partition

1) linear O(n) time, no extra memory [see next video]
2) reduceds problem size


Partitioning Subroutine Implementation

Easy Way Out: using O(n) w/ extra memory, easy to partition around pivot in O(n) time

[3, 8, 2, 5, 1, 4, 7, 6]

Linear scan and bucket less than and greater than

Add to the left (idx ++) and to the right (idx --) of a copy of the allocated array


In-Place Impelementation

Assume: pivot = 1st element of the array

NOTE:
[if not, swap pivot <-> 1st element as preprocessing step]

High-Level Idea: two groups, what we've seen and what we haven't seen

Split what we've seen into [less than, greater than]

Keep pivot at idx 0 until the very end

all elements less than the pivot come first, all elements greater than the pivot come last

- single scan through array
- invariant: everything looked at so far is partitioned

Keep track of:
	- boundary of what we have looked at / haven't looked at (j)
	- where is the split for less than / greater than pivot (i)
	
Each step must advance j to maintain linear O(n)

The first examined element is indeed partitioned (no comparison necessary)

If the first element is bigger then pivot, i remains the same

If any subsequent element is smaller, we must swap (another routine, perhaps)


==============================================================================
Choosing a Good Pivot


Running time of QuickSort depends on the quality of the pivot


If we implement QuickSort so that ChoosePivot always selects the first element of the array, the running time is quadratic on an input array that is already sorted (O(n^2)), which is VERY BAD for a sorting algorithm

The lower bound on this algorithm is quadratic because we recurse on every element in the array (using this method of choosing the pivot)



We ideally want the median element (n/2 split for recursion) - best case scenario

If in every recursive call we get a 50/50 split, the running time is O(nlogn)

The recurrence matches the recurrence that governs MergeSort (nlogn)

Since the recursive calls are at most n/2, T(n) <= 2T(n/2)

Subproblems shrink at the size of subproblem proliferation

Since we have a tight recurrence on partition (theta n) we actually get a tighter estimate (theta nlogn) in this magical scenario of perfect pivots


Random Pivots

QuickSort is the first example of randomized algorithms (e.g. flipping a coin to get performance)

Big Idea: RANDOM PIVOTS

Each input in array of length k has the same odds of selection

This algorithm will run differently on the same input on different runs

In the 1970s it was discovered that randomness could make solving certain problems very fast, more elegant and easier to maintain

Hope: a random pivot is "pretty good" "often enough"

Any pivot that gets us a reasonable split (close to n/2) will yield nlogn time


Two intuitive points:

1: A 25/75 split is defined as good enough for nlogn (claim by Roughgarden)

Because we do not have balanced sub-problems, you cannot use Master Method to analyze this

2: We don't have to get that lucky to get a 25/75 split
i.e. in an array of 1-100 we can pick anything such that 25 >= x >= 75

50% probability we get a split that is "good enough"



Mathematical Analysis
If you really want to know if an idea is good or bad, you need mathematical analysis


Quicksort Theorem: For every input array of length n, the average running time of QuickSort (with random pivots) is O(nlogn)

NOTE: every input (no assumptions about the data)

- recall our guiding principles - general principles with great guarantees independent of the data

If you have a dataset that fits in the memory of your machine, sorting is a computational primitive - QuickSort runs fast enough that you can think of it as "for free"

Go ahead and sort the array - it might make your life easier in other algorithmic or computational problems

NOTE: "average" is over random choices made by the algorithm 
(i.e. pivot choices)

We've seen QuickSort vary between upper bound O(n^2) and lower bound O(nlogn). What is amazing about QuickSort, you are almost never going to see anything but the lower bound thanks to randomization

============================================================================
Probability Review

Topics Covered
	Sample Spaces
	Events 
	Random Variables
	Expectation - average value of a random variable
	Linearity of Expectation - very important property
	
See also:
	Lehman-Leighton notes (free PDF)
	wikibook on discrete probability
	

SAMPLE SPACES

"all possible outcomes"

Big Omega = sample space

(my notes big omega = OM())

in algorithms, Big Omega is usually finite (therefore discrete probability)

also: each outcome Big Omega has a probability p(i) >= 0
	- probability of each possible outcome should be non-negative
	- constraint: the sum of the probability should be one
		at least one thing should happen

Example #1: Rolling 2 dice Big Omega (36 possible outcomes)
	Each outcome should be equally likely, 1/36
	
Example #2: Choosing a random pivot in outer QuickSort call
	BigOmega{1, 2, ... n}
	and p(i) = 1/n for all OM()
	

EVENTS
an event is a subset of all the things that could happen
an event is a subset of OM


probability of an event is sum of probabilities of all the outcomes contained in that event

example event "the sum of two dice is 7"

S { 
	(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)
}

6 members of the 36 possible outcomes are contained in the definition of this event

1/36 probability for each outcome, and the event probability equals the sum of the various outcomes included in the event definition, so:


1/36 * 6 outcomes in event S = 6/36 or 1/6 probability of this event (sum = 7)



event example 2:

randomly chosent pivot gives a 25-75 split or better

looking for a reasonably good split, both sides of the pivot have at least 25% 

50% probability to get a 25/75 split or better

if array had 1-100, we want both subarrays to be at least len(25)

if we choose the element that is 26 or bigger in value, then the left side will have at least 25

if we choose the element that is 75 or smaller will give 25 elements on the right side

anything in the middle 50% is acceptable as a pivot choice, therefore giving a 1/2 probability

S = {
	((n/4) + 1)th smallest element, ..., (3n/4)th smallest element
}

Pr[S] = n/2 / n

cardinality (n/2) times the likelihood of each of the outcomes (n/1, or n, since each of the outcomes is equally likely)


Randomized QuickSort has good pivots 1/2 the time, which helps give intuition as to why it is fast


RANDOM VARIABLES

Some statistic that measures what happens in the random outcome

A random variable X is a real-valued function defined on the sample space OM

Given an outcome (random outcome) this gives you a number


Ex #1: sum of the two dice (number representing the random outcome of both dice)

Ex #2: size of subarray passed to the 1st recursive call

number of elements in the input array smaller than the pivot 



EXPECTATION

the average

let X:OM -> IR be a random variable

The expectation E[X] (aka expected value) of X = average value of X

(weighed against the probability of the various outcomes)

 
Math: sum of everything that could happen (i = one possible outcome)
		i	
	get the value of this random value when that outcome occurs
		X(i)
	weight it times the probability of that outcome occuring
		P(i)
		
=Sigma X(i) * P(i)



Quiz 3: What is the expectation of the sum of two dice?

the average value is 7

use linearity of expectation (which we have not covered)


Quiz 4: Which of the folling is closest to the expectation of the size of the subarray passed to the first recurisve call in QuickSort?

n/2

technically n-1/2

expected value of both recursive calls is equal, and it is the first recursive call


let X = subarray size
// use expectation to solve
Then E[X] = 1/n * 0 + 1/n * 1

1/n * 0: minimum of pivot resulting in zero elements to the first recursive call
1/n * 1: second-smallest of pivot resulting in one elmenet to the first recursive call
...
1/n * (n-1)
	
(didn't really make sense)



LINEARITY OF EXPECTATION

Common in analysing random processes

Claim [LIN EXP]: let x1, ..., xn be random variables
	defined on OM (same sample space)
	
	Then: 
		the expected value of a sum of random variables
		is equal to the sum of the expectations of the individual random 			variables
		
	CRUCIAL: Holds even when the random variables are not independent
	
The same would not be true for products, only for sums

EX #1: x1, x2 for results of first and second die

E[Xj] = 1/6 (1 + 2 +3 +4 +5 + 6) = 3.5

By lin exp, the expected value of two die is simply double; 7

E[X1] + E[x2] = E[x1] + E[x2] = 3.5 + 3.5 = 7


Simple to prove and unbelievably useful



Example: Load Balancing

assigning processes to servers

Problem: need to assign n processes to n servers

Proposed Solution: assign each process to a random server

Question: what is expected number of processes assigned to a server?

Load Balancing Solution:

	Sample space OM = all n^n assignments of processes to servers (n choices for 		each of the n processes, each equally likely since chosen at random)

	Let Y = total number of processes assigned to the first server
	
	Goal: E[Y] - expected number of processes assigned to a server
	
	Instead of computing the sum by enumerating all outcomes, compute the 		expecatation for one and then sum for all n
	
	For given process, j:
	Compute Xj = {1 if jth process assigned to first server, 0 otherwise}
	
	// Xj, example of an indicator variable
	
	(whether or not the jth process gets assigned to the first server)
	
	Observe: the total number of processes that gets assigned to the first 			server is the sum from j=1 to n of Xj
	
	Win: We only have to compute the expectation of a single variable, Xj 
		A very simple indicator variable
		
	E[Y] = E[sum(Xj)]  // since y = sum(xj)
	
	// Linearity of expectation
	
	sum(Pr[Xj = 0] * 0 + Pr[xj = 1] * 1)
	
	***
	so probability that jth process gets assigned to the first server is 1/n
	where n is the number of servers "competing" randomly for that process
	***
	
	We then sum 1/n, n times; which is just 1!
	
	Because there are both n servers and n processes to delegate, each server 			has an expected number of processes: 1
	

===========================================================================
Review (pt II)

Topics:
	Conditional Probability
	Independence of Events
	

CONDITIONAL PROBABILITY

Sameple Space OM = "all possible outcomes"
	in algorithms, OM is usually finite

Each outcome i E OM has a probability p(i) >= 0
	where i represents a single outcome
	
	Constraint: E p(i) = 1  // all the probabilities sum to 100%
	
An event is a subset S <= OM 

The probability of an event S is E (iES) p(i)
	sum of probabilities where i is within S sample space subset
	
	probability of all of outcomes that the event contains
	

CONDITIONAL PROBABILITY

Let x,y <= OM be events (both events of the same sample space)

x and y could be disjoint (no intersection) or they could have a non-trivial intersection

x and y need not cover all of omega

Then: Pr[X | Y]  // probability of x given y; something in y happened
 	=	
 		Pr[X intersect Y]/ Pr[Y]
 		// this is by definition the probability of x given y
 		
 
 Quiz 1:
 Suppose you roll two fair dice; what is the proabability that at least one die is a 1, given that the sum of the two dice is 7?
 
 1/3
 
 Two events: 
 x = at least one die is a 1
 y = sum of two dice is a seven
 
 y = {(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)}
 
 Pr[X intersect Y] / Pr[Y]
 
 X intersect Y is (1,6), (6,1)
 
 X intersect Y = 2/36
 Y = 6/36
 
 2/36 / 6/36 = 1/3
 
 
 
 INDEPENDENCE OF EVENTS
 
 Definition: events x, y <= OM are independent if and only if
 
 Pr[X intersect Y] = Pr[X] * Pr[Y]
 
 you check: this holds if and only if Pr[X | Y] = Pr[X]
 
 knowing that y happened gives you no information about whether X happened
 
 Pr[Y|X] = Pr[Y]
 

WARNING: indepedence is a very subtle concept;
	your intuition about independence is very often wrong
	no other source causes so many bugs in proofs by researchers as
		misunderstandings of independence and using intuition
		instead of formal definition
		
Rule of Thumb: if things are independent by construction, then you can proceed that they are indepedent

If there is any doubt about independence, then assume dependence


INDEPENDENCE OF RANDOM VARIABLES

Definition: random variables a, b (both defined on OM) are independent 

The events Pr[A = a], Pr[B = b] are independent for all a, b

Expectations just multiply in this case!

(this ONLY holds for independent random variables)

Pr[A = a and B = b] = Pr[A = a] * Pr[B = b]

The probability of both happening is simply equal to the indidividual probability of each happening

Claim: if A, B are independent, then E[A*B] = E[A] * E[B]


because a and b are independent this probabilty factors (multiplies), which extends linearity of expectation (which allows simply for summing)


EXAMPLE

Let x1, x2 = sig(0,1) be random, and x3 = x1 + x2  XOR (exclusive or, 1 if 1,0 pair)

x1 and x2 are equally likely to be 1 or 0

Formally OM = { 00 0, 10 1, 01 1, 11 0}, each equally likely

Claim: X1 and X3 are independent


============================================================================
Analysis I: A Decomposition Principle


For a worst-case input of length n, the average running time of QuickSort (with random pivots) is O(nlogn)

Average running time is far closer to the best cast scenario (nlogn) than the worst case (n^2)


PRELIMINARIES

Fix input array A of length n.

Sample size OM = all possible outcomes of random choice in QuickSort (i.e. pivot sequences)

Key Random Variable: (computing expectation of) for o E OM (given pivot sequence)

	pivot sequence (sigma)

	C (sig) = # of comparisons between two input elements made by QuickSort (given random choices sigma)

	comparison = between two different entries (e.g. is the 3rd > 7th entry?)

Lemma: running time of QuickSort dominated by comparisons

	formally: constant (c) such that:
		total operations of any type of what QuickSort executes is higher by a constant factor than c

Remaining goal: E[c] = O(nlogn)

	Prove that the average number of comparisons is O(nlogn)

	Prove the expectation of the random variable c (total comparisons) is bounded by O(nlogn)

Decomposition Principle: c == total comparisons, it is a complicated variable, however (bounded by n^2 and nlogn)


BUILDING BLOCKS

Master method does not apply

1. Size of the two subproblems is random (random, unbalanced subproblems)

	Could be as good as a perfectly balanced split, or as bad as a 0, 99 split

	You are not going to get all subproblems with the same size unless you are extremely lucky

2. All subproblems will not have the same size (not going to happen in QuickSort unless you are very lucky)

Decomposition Principle: take a complicated random variable, break it down and make it simple, and use the simple variable to solve the complicated problem

Building Blocks:

	[A = fixed input array, of length n]

Notation: Zi = ith smallest element of A

	ith order statistic

	IS NOT the element of the ith position of the A unsorted array

	Once we sort the array, only then, is it the ith index

	[6, 8 ,10 ,2]

	z1 = 2
	z2 = 6
	z3 = 8
	z4 = 10


Simpler Random Variables

	Given choice of pivots (sig), and given choices i, j (between 1 < n, and given i < j)

	Xij(sig) = # of tims zi, zj get compared in QuickSort

		i.e. how many times do these two elements get compared as we execute QuickSort?


QUIZ: Fix two elements of the input array. How many times can these two elements get compared with each other during the execution of QuickSort?

0 or 1

The only place where comparisons between elements happens, is in the Partition() subroutine

The pivot element is compared to each other element of the subarray only ONCE

	Every single comparison involves the pivot element

One of these elements MUST be the pivot in order for a pair of elements to be compared

The pivot is then excluded from both recursive calls, therefore once a pair is compared they will not be compared again!

Thus: each Xij is an "indicator" (i.e. 0-1) random variable


A DECOMPOSITION APPROACH

So:	

	C(sig) = # of comparisons between input elements

	Xij = # of comparisons between Zi and Zj

Thus:
	
	Every comparison involves one pair of elements (i, j)

		Therefore, we can write C as a sum of the Xijs

	Double Sum = iterate over all ordered pairs ij, where i and j are between 1 and n, and where i < j

		Easy way to write that iteration

	no matter what the chosen pairs are we have the equality

		A(sig), C(sig) = SUM(i => n-1)SUM(j => n) Xij(sig)

	By linearity of expectation: E[C] = SUM(i => n-1)SUM(j => n) E[Xij]

		Power: turning E[C] (complicated) into the double sum of the Expected Value of E[Xij] random variable

	Since E[Xij] = 0 * Pr[Xij = 0] + 1 * Pr[Xij = 1] = Pr[Xij = 1]

		^expanded definition of expectation

		possibility it takes on the value 0, plus the possibility it takes on the value one

		>>Expected value of Xij is just the probability Xij = 1!

	Thus: E[C] = SUM(i = 1 => n -1) SUM(j = i + 1 => n) Pr[Zi, Zj get compared]

		iterate over all ordered pairs

		and multiply by the probability that Zi, Zj get compared

	Next video will nail the probability that Zi, Zj get compared


(NOTE: this is a pratical DECOMPOSITION PRINCIPLE example)

A General Decomposition Principle

1. Identify random variable Y that you really care about.

	We knew based on the lemma that we cared about the # of comparisons

2. Express Y as a sum of indicator random variables:

	Y = SUM(l = 1 => m) Xe

3. Apply linearity of expectation:

	E[Y] = SUM(l = 1 => m) Pr[Xl = 1]

Upshot: to understand the complicated left hand side, understand the simpler probability of these (Pr[]) various events

	In QuickSort, the probability Xi and Xj get compared

=========================================================================================
Analysis II: The Key Insight


Recall:

	C(sig) = # of comparisons QuickSort makes with pivots sig

	Xij(sig) = # of times zi + zj get compared

	zi + zj denote the ith and jth smallest elements of the subarray, respectively

	Every comparison involves sum(zi) sum(zj), therefore we can express 
	
		E[C] = SUM(i = 1 => n -1)SUM(j = i-1 => n) Pr[Xij = 1]


KEY CLAIM: Pr[Zi, Zj get compared] = 2 / j - i + 1

	So long as i < j (e.g. V i < j)



PROOF OF KEY CLAIM:

	Fix Zi, Zj with i < j

	Consider the set Zi, Zi+1, ..., Zj-1, Zj

	Recall the initial array is not sorted, these elements we are considering are NOT contiguous

Inductively:

	These j - i + 1 elements live parellel lives in the following sense

	Inductively:

		As long as none of these are chosen as a pivot, all are passed to the same recursive call

			"Living in harmony"

	
	Consider the first among Zi ... Zj that gets chosen as pivot

		1: if Zi or Zj gets chosen first, they are definitely compared

		2: if Zi+1 or Zj -1 gets chosen first, then they are definitely NOT compared

			When you choose a pivot and partition an array, all the comparisons involve the pivot (two elements not of the pivot do not get compared)

			Because the pivot comes between them, Zi and Zj will be split into separate recursive calls

	These two observations make up the "Key Insight" in the QuickSort algorithm


PROOF OF KEY CLAIM (con'd)

1: Zi or Zj chosen first => they get compared
2: One of Zi+1 ... Zj-1 chosen first => Zi Zj never compared

Note: since pivots always chosed uniformly at random, each of Zi ... Zj is equally likely to be the first

=> Pr[Zi, Zj get compared] = 2 / (j - 1 + 1)

2 = choices that lead to case 1

(j - i + 1) = total # of choices

	RECALL: Zi ... Zj

	Zi = smaller member of an array
	Zj = larger memeber of an array (when sorted ASC)

	Ex: If i and j are one after the other (e.g. 3 and 4), they have a 100% probability of being compared because
		we recurse Partition until the base case of 1, so ***elements that are adjecent in value are destined to be compared***

	***Elements that are NOT adjacent in value (as an expression of Z[], the sorted array) have a probability of NEVER being compared***

		This probability of non-comparison increases for each value in-between the given pair i, j

So: E[C] = SUM(i = 1 => n-1) SUM(j = i + 1 => n) 2 / (j - i + 1)

====================================================================================
Analysis III: Final Calculations


C = # of comparisons QC makes

Express C as sum of indicator variables for each variable Xij
	- Counting the number of comparisons involving the ith smallest and jth smallest entries in the array (either 0 or 1 total comparisons)

Comparison probability for ith and jth smallest element in the array, the probability is 2 / (j - i + 1)


E[C] = 2 * SUM(i = 1 < n-1) SUM(j = i+1 < n) 1/(j-i+1)

	2 / (j - i + 1) translates by pulling the 2 out front as a leading constant (suppressed in big O notation)


Prove the number of comparisons is bounded by O(nlogn)

Biggest "sum-and" will be when i and j are right next to each other (j is one bigger than i)
	sum-ands big as one half, with a quadradic number of terms

Using a sum-and value of 1/2 and a quadradic number of terms, we could very easily derive an upper bound that is quadradic in N

We are going to have to be more clever in how we evaluate the double-sum)

	Think about a fixed value of i in the outermost sum, and then ask how big can the innermost sum be?

	NOTE: for each fixed i, the inner sum is SUM(j = i+1 => n) * 1/(j-1 + ) 

		As j gets bigger, the denominator gets bigger and sum-ands get smaller

		The biggest sum is when j is the smallest possible (namely, i + 1)

		When j is i + 1, the sum-and is 1/2

		Every inner sum will run from 1/2 ... 1/n

	Lets take the initial expression and bound it by a single upper sum

So: 

	n choices for i

	E[C] <= 2 * n * SUM(k = 2 => n) 1 / k

	The one of the inner sums is the one occurring when i = 1

	Therefore, make a change of variables and express the upper bound on each inner sum
	
		as the sum from K equal two to N of one over K

	The single sum makes it much more manageable


	CLAIM: SUM(k = 2 => n) 1/k cannot be very big, it is only logarithmic in n

	This will complete the proof by giving us an overall upper bound of 2 * n * log(n)

	nlogn bound with quite-reasonable constants (namely, 2)


COMPLETING THE PROOF

E[C] <= 2n SUM(k=2 => n) 1/k

PROOF OF CLAIM:

(graph with points corresponding to fractions of the form 1/k)

e.g. if you were to graph 1/k

k on x and height 1/k

1,1
1,1/2
1,1/3

f(x) = 1/x kisses the corners of each of the rectangles made by the graph

The sum can be thought of as the area in all the rectangles, ***upper bounded by the area under the curve defined by 1/x***

The sum(k=2 => n) 1/k is bounded by the integral (1 => n) 1/x dx (area under the curve)

1/x = ln(x)

Therefore, since the integral is from 1 => n, the area under the curve (the bound for 1/k)
	can be represented as ln(n) - ln(1)

	the natural log of 1 is 0 (ln(1) = 0), so therefore we are left with the natural log of n (ln(n))


Therefore, we have proven the claim that the SUM(k = 2 => n) 1/k is at most logarithmic

With that expression being logarithmic, we have the final upper computational bound of 2 * n * ln(n)

	O(nlogn) with "quite reasonable" constants

	This is the number of comparisons only, the running time is dominated by comparisons

	Moreover, QuickSort runs in place, so its added computational load outside of comparisons is minimal

